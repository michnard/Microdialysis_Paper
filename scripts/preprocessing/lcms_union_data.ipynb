{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c6f7dd",
   "metadata": {},
   "source": [
    "Consider the union of all lcms compounds across the three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d882b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c21b251c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of individual, high quality compounds per animal:  [241, 343, 345]\n",
      "Number of total, high quality compounds (union):  482\n"
     ]
    }
   ],
   "source": [
    "# load LCMS data\n",
    "lcms_dir = '../../data/raw/lcms/'\n",
    "lcms_files = [f for f in os.listdir(lcms_dir) if f.startswith('dansyl')]\n",
    "# read data\n",
    "dfs = [pd.read_csv(os.path.join(lcms_dir, f)) for f in lcms_files]\n",
    "## Step 1: select compounds with high confidence identification (only Tier 1 & 2)\n",
    "tier_dfs = [df[(df[\"Identification Level\"] == \"Tier 1\") | (df[\"Identification Level\"] == \"Tier 2\")].copy() for df in dfs]\n",
    "# merge compound names of last dataset\n",
    "dfs[2]['Compound'] = dfs[2]['Compound'].fillna(dfs[2]['Compound.1'])\n",
    "tier_dfs[2]['Compound'] = tier_dfs[2]['Compound'].fillna(tier_dfs[2]['Compound.1'])\n",
    "# extract compounds names\n",
    "compounds = [df[\"Compound\"].tolist() for df in tier_dfs]\n",
    "print('Number of individual, high quality compounds per animal: ', [len(c) for c in compounds])\n",
    "# find intersection of compounds in all 3 datasets\n",
    "common_comps = list(set(compounds[0]).intersection(set(compounds[1])).intersection(set(compounds[2])))\n",
    "common_comps.sort()\n",
    "# find the group of total compounds -- union\n",
    "union_comps = list(set(compounds[0]).union(set(compounds[1])).union(set(compounds[2])))\n",
    "print('Number of total, high quality compounds (union): ', len(union_comps))\n",
    "# order datasets according to union comp\n",
    "data_order = []\n",
    "for i in range(3):\n",
    "    data_order.append(dfs[i][dfs[i]['Compound'].isin(union_comps)].copy())\n",
    "    data_order[i] = data_order[i].set_index('Compound').reindex(union_comps).reset_index()\n",
    "    # if not in Tier 1 or Tier 2, set to NaN\n",
    "    data_order[i].loc[~data_order[i]['Compound'].isin(compounds[i]), data_order[i].columns != 'Compound'] = np.nan\n",
    "# order so as to have common compounds first\n",
    "data_order = [pd.concat([td[td[\"Compound\"].isin(common_comps)], td[~td[\"Compound\"].isin(common_comps)]]) for td in data_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a34460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing steps\n",
    "log_transform = True\n",
    "# data_order_order extraction and columns renaming (specific for data_orderset of day 1)\n",
    "metadata_order = [d.iloc[:,:21] for d in data_order]\n",
    "data_order[0] = data_order[0].iloc[:,21:-4]\n",
    "if log_transform:\n",
    "    data_order[0] = data_order[0].transform(lambda x: np.log2(x))\n",
    "new_columns = []\n",
    "for col in data_order[0].columns:\n",
    "    name = col[-6:]\n",
    "    if name[0] == '_':\n",
    "        name = name[1:]\n",
    "    new_columns.append(name)\n",
    "data_order[0].columns = new_columns\n",
    "\n",
    "# columns renaming (specific for data_orderset of day 2)\n",
    "data_order[1] = data_order[1].iloc[:,21:-3]\n",
    "if log_transform:\n",
    "    data_order[1] = data_order[1].transform(lambda x: np.log2(x))\n",
    "new_columns = []\n",
    "for col in data_order[1].columns:\n",
    "    parts = col.split('.')\n",
    "    if len(parts) >= 2:\n",
    "        new_col = f\"{parts[0]}{int(float(parts[1])/2)+1}_{0}{int(float(parts[1])%2+1)}\"\n",
    "    else:\n",
    "        new_col = col+\"1_01\"  # Default to _01 if no replicate info\n",
    "    new_columns.append(new_col)\n",
    "data_order[1].columns = new_columns\n",
    "\n",
    "# columns renaming (specific for data_orderset of day 3)\n",
    "data_order[2] = data_order[2].iloc[:, 24:-3]\n",
    "if log_transform:\n",
    "    data_order[2] = data_order[2].transform(lambda x: np.log2(x))\n",
    "new_columns = []\n",
    "for icol in np.arange(0,data_order[2].shape[1],2):\n",
    "    new_columns.append(f\"{int(icol/2)+1}_01\")\n",
    "    new_columns.append(f\"{int(icol/2)+1}_02\")\n",
    "data_order[2].columns = new_columns\n",
    "\n",
    "\n",
    "# check for columns with abnormally high or low values across all datasets\n",
    "# --> remove samples with intensities out of bounds (median ± 4×MAD)\n",
    "# --> that was computed already in data_preprocessing_alignment, value is np.float64(-0.7805087050586059)\n",
    "\n",
    "for d in data_order:\n",
    "    for col in d.columns:\n",
    "        if d[col].median() < -0.7805087050586059:\n",
    "            d[col] = np.nan\n",
    "\n",
    "# remove dirty technical replicates\n",
    "def clean_row(row, thr=1):\n",
    "    # 1) compute abs difference of each consecutive pair\n",
    "    row_clean = []\n",
    "    for i in range(1, len(row), 2):\n",
    "        diff = np.abs(row[i] - row[i-1])\n",
    "        if diff > thr:\n",
    "            row_clean.append(np.nan)\n",
    "        else:\n",
    "            row_clean.append(np.mean(row[i-1:i+1]))\n",
    "    return np.array(row_clean)\n",
    "\n",
    "# do the cleaning\n",
    "clean_data = []\n",
    "for id, d in enumerate(data_order):\n",
    "    clean_data.append([])\n",
    "    for i in range(d.shape[0]):\n",
    "        clean_data[id].append(clean_row(d.iloc[i,:].values, thr=1))\n",
    "    clean_data[id] = np.array(clean_data[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c64bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../data/processed/lcms_union_clean_data.npy', {'data_day1': clean_data[0], 'data_day2': clean_data[1], 'data_day3': clean_data[2]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MD_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
